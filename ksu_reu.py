# -*- coding: utf-8 -*-
"""KSU_REU.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JZxp_USDXVpRUFcg_zrgjKBQhJqVpTcX

**Kennesaw State University REU Site**
***
This project's goal is to train a sensor-driven AI model to correctly detect and categorize emotions from the CREMA-D Dataset. The emotions in this dataset are sadness, anger, disgust, fear, happy, and neutral. This project will utilize a Long Short-Term Memory (LSTM) AI. This is a special kind of Recurrent Neural Network (RNN). It is very good at understanding sequences of data over time, i.e., speech, audio, or music. LSTM is very good at remembering information from early on in the sequence, especially compared to other neural networks. We want to use LSTM because tone and pitch are some of the features used in emotion detection, and they will change throughout the sequence.
"""

#Imports for Audio Processing
import librosa
import librosa.display
from librosa.effects import time_stretch as _ts, pitch_shift as _ps
from IPython.display import Audio

#Imports for Data Processing
import pandas as pd
import numpy as np
import os
from sklearn.model_selection import train_test_split, GroupShuffleSplit
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import compute_class_weight
from collections import Counter, defaultdict

#Imports for LSTM and Training
import torch
import torch.nn as nn
import torch.optim as optim
import torchaudio
import torchaudio.sox_effects as sox
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from torch.nn.utils.rnn import pad_sequence
from torch.optim.lr_scheduler import OneCycleLR
import torch.nn.functional as F

#Import for visualization  and random seeds
import matplotlib.pyplot as plt
import random
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, f1_score, accuracy_score

# Commented out IPython magic to ensure Python compatibility.
# %pip install -q transformers
from transformers import Wav2Vec2Processor, Wav2Vec2Model

#Import Zip Files
import zipfile
from google.colab import drive
#Mount Drive
drive.mount('/content/drive')
zip_path = '/content/drive/MyDrive/Crema-D.zip'
extract_path = '/content/data'
#Unzip and explore the data
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
     zip_ref.extractall(extract_path)

data_path = os.path.join(extract_path, 'AudioWAV')
file_list = os.listdir(data_path)
print(file_list[:5])

#Mixup Augmentation
def mixup_data(x, y, alpha = 0.4, device = 'cpu'):
    '''Returns mixed inputs, pairs of targets, and lambda'''
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1.0

    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(device)

    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

def mixup_criterion(criterion, pred, y_a, y_b, lam):
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

class FocalLoss(nn.Module):
    def __init__(self, gamma = 2.0, weight = None, reduction = 'mean'):
        super().__init__()
        self.gamma = gamma
        self.weight = weight
        self.reduction = reduction

    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs,
                                  targets,
                                  weight = self.weight,
                                  reduction = 'none')
        p_t = torch.exp(-ce_loss)
        loss = (1 - p_t) ** self.gamma * ce_loss
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss

#Separate the Emotions
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base")
processor.feature_extractor.sampling_rate = 16000
wav2vec2 = (Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base").to(device).eval())
wav2vec2.config.gradient_checkpointing = False
print(f"Using device: {device}")
emotion_map = {
    'ANG': 'anger', 'DIS': 'disgust', 'FEA': 'fear',
    'HAP': 'happy',  'NEU': 'neutral','SAD': 'sadness'
}

#Augmentation Function
def augment_audio_features(audio, sr = 16000):
    augment_type = random.choices(
        ['none', 'pitch', 'stretch', 'noise']
    )
    if augment_type == 'pitch':
        n_steps = random.uniform(-1, 1)
        audio = librosa.effects.pitch_shift(audio, sr = sr, n_steps = n_steps)
    elif augment_type == 'stretch':
        rate = random.uniform(0.9, 1.1)
        audio = librosa.effects.time_stretch(audio, rate = rate)
    elif augment_type == 'noise':
        noise_amp = 0.002 * np.random.uniform() * np.amax(audio)
        audio += noise_amp * np.random.normal(size = audio.shape[0])
    return audio


def spec_augment( feats,
                  num_time_masks = 2, max_time_frax = 0.1,
                  num_freq_masks = 2, max_freq_frax = 0.1):
    feats = feats.copy()
    T, F = feats.shape
    for _ in range(num_time_masks):
          t = int(random.random() * max_time_frax * T)
          t0 = random.randint(0, max(0, T-t))
          feats[t0:t0+t, :] = 0

    for _ in range(num_freq_masks):
          f = int(random.random() * max_freq_frax * F)
          f0 = random.randint(0, max(0, F-f))
          feats[:, f0:f0+f] = 0
    return feats


#Extract Features
def extract_features(file_name, augment=False, spec_p=0.5):
    # 1) Load & optional waveform augment
    audio, sr = librosa.load(file_name, sr=16000)
    if augment:
        audio = augment_audio_features(audio, sr)

    # 2) Tokenize & extract Wav2Vec2 embeddings
    inputs = processor(
        audio,
        sampling_rate=processor.feature_extractor.sampling_rate,
        return_tensors="pt",
        padding=True
    )
    with torch.no_grad():
        outputs = wav2vec2(inputs.input_values.to(device))

    # 3) Move to CPU and convert to NumPy **once**
    #    Before this line: a torch.Tensor on GPU
    features = outputs.last_hidden_state.squeeze(0).cpu().numpy()
    #    Now features is a NumPy ndarray

    # 4) Apply spec_augment on the NumPy array
    if random.random() < spec_p:
        features = spec_augment(features)

    # 5) Return the NumPy array (no further .cpu()/.numpy() calls)
    return features

#Build Dataset
embedding_dir = '/content/data/wav2vec2_embeddings'
os.makedirs(embedding_dir, exist_ok=True)

features = []  # stores paths to .npy files
labels = []    # emotion label strings
speakers = []  # speaker IDs

for filename in file_list:
    if filename.endswith('.wav'):
        spk = filename.split('_')[0]
        code = filename.split('_')[2]
        if code in emotion_map:
            path = os.path.join(data_path, filename)
            try:
                # Extract mean-pooled wav2vec2 embedding
                audio, sr = librosa.load(path, sr=16000)
                inputs = processor(audio, sampling_rate=16000, return_tensors="pt", padding=True)
                with torch.no_grad():
                    outputs = wav2vec2(inputs.input_values.to(device))
                features_np = outputs.last_hidden_state.squeeze(0).cpu().numpy().mean(axis=0)  # shape: (768,)

                # Save embedding as .npy
                emb_path = os.path.join(embedding_dir, filename.replace('.wav', '.npy'))
                np.save(emb_path, features_np)

                features.append(emb_path)
                labels.append(emotion_map[code])
                speakers.append(spk)
            except Exception as e:
                print(f"Error with {filename}: {e}")

print(f"Total samples: {len(features)}")
print(Counter(labels))

#Encode Labels
le = LabelEncoder()
labels_encoded = le.fit_transform(labels)

#Split Data into train, validation, and test sets
gss = GroupShuffleSplit(n_splits = 1, test_size = 0.3, random_state = 72)
train_idx, temp_idx = next(gss.split(features, labels_encoded, groups = speakers))

X_train_raw = [features[i] for i in train_idx]
y_train = [labels_encoded[i] for i in train_idx]
spk_temp = [speakers[i] for i in temp_idx]
X_temp_raw = [features[i] for i in temp_idx]
y_temp = [labels_encoded[i] for i in temp_idx]

gss2 = GroupShuffleSplit(n_splits = 1, test_size = 0.5, random_state = 72)
val_idx, test_idx = next(gss2.split(X_temp_raw, y_temp, groups = spk_temp))

X_val_raw = [X_temp_raw[i] for i in val_idx]
y_val = [y_temp[i] for i in val_idx]
X_test_raw = [X_temp_raw[i] for i in test_idx]
y_test = [y_temp[i] for i in test_idx]

# Compute normalization stats on training set
train_embeddings = [np.load(path) for path in X_train_raw]
mean = np.mean(train_embeddings, axis=0)
std = np.std(train_embeddings, axis=0) + 1e-8

# Dataset
class EmbeddingDataset(Dataset):
    def __init__(self, file_paths, labels, mean, std):
        self.files = file_paths
        self.labels = labels
        self.mean = mean
        self.std = std
    def __len__(self):
        return len(self.files)
    def __getitem__(self, idx):
        feats = np.load(self.files[idx])
        feats = (feats - self.mean) / self.std
        return torch.tensor(feats, dtype=torch.float32), self.labels[idx]

# Loaders
train_set = EmbeddingDataset(X_train_raw, y_train, mean, std)
val_set = EmbeddingDataset(X_val_raw, y_val, mean, std)
test_set = EmbeddingDataset(X_test_raw, y_test, mean, std)

train_loader = DataLoader(train_set, batch_size=32, shuffle=True)
val_loader = DataLoader(val_set, batch_size=32, shuffle=False)
test_loader = DataLoader(test_set, batch_size=32, shuffle=False)

# Class weights
class_weights = compute_class_weight('balanced', classes=np.unique(labels_encoded), y=labels_encoded)
class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)

#Oversampling in DataLoader
#Compute how many samples per class
class_counts = np.bincount(y_train)
class_sample_weights = 1.0 / class_counts
sample_weights = [class_sample_weights[label] for label in y_train]
sampler = WeightedRandomSampler(
    weights = sample_weights,
    num_samples = len(sample_weights),
    replacement = True
)

#PyTorch-Compatible Dataset Class
# AudioDataset for precomputed .npy mean-pooled Wav2Vec2 features
class AudioDataset(Dataset):
    def __init__(self, file_paths, labels):
        self.files = file_paths
        self.labels = labels
    def __len__(self):
        return len(self.files)
    def __getitem__(self, idx):
        feats = np.load(self.files[idx])        # feats: [T, 768]
        return feats, self.labels[idx]

# Collate function: just stack features and labels, no padding needed
def collate_fn(batch):
    sequences = [torch.tensor(x, dtype=torch.float32) for x, _ in batch]  # [T, 768]
    sequences_padded = pad_sequence(sequences, batch_first=True)          # [batch, max_T, 768]
    labels = torch.tensor([y for _, y in batch], dtype=torch.long)
    return sequences_padded, labels

# Instantiate datasets (make sure mean & std computed from training set!)
val_set   = AudioDataset(X_val_raw, y_val)
test_set  = AudioDataset(X_test_raw, y_test)
train_set = AudioDataset(X_train_raw, y_train)
train_loader = DataLoader(train_set, batch_size=32, shuffle=True, collate_fn=collate_fn)
# Dataloaders
val_loader = DataLoader(
    val_set,
    batch_size = 32,
    shuffle = False,
    collate_fn = collate_fn
)
test_loader = DataLoader(
    test_set,
    batch_size = 32,
    shuffle = False,
    collate_fn = collate_fn
)

#Define the LSTM Model
class EmotionLSTM(nn.Module):
    def __init__(self, input_size = 61, hidden_size = 96, num_layers = 2, dropout = 0.5, num_classes = 6):
        super(EmotionLSTM, self).__init__()
        self.lstm = nn.LSTM(
            input_size = input_size,
            hidden_size = hidden_size,
            num_layers = num_layers,
            batch_first = True,
            dropout = dropout,
            bidirectional = True)
        self.bn = nn.BatchNorm1d(hidden_size * 4)
        self.fc1 = nn.Linear(hidden_size*4, 64)
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(64, num_classes)

    def forward(self, x):
        out, _ = self.lstm(x) #Shape: (batch, time_steps, hidden_size * 2)
        out_mean = torch.mean(out, dim = 1)
        out_max = torch.max(out, dim = 1)[0]  # Take only values from torch.max
        out = torch.cat((out_mean, out_max), dim=1) # Concatenate mean and max
        out = self.bn(out)
        out = torch.relu(self.fc1(out))
        out = self.dropout(out)
        out = self.fc2(out) #Shape: (batch, num_classes)
        return out

#Hybrid CNN-LSTM model
class ConvLSTM(nn.Module):
    def __init__(self,
                 in_feats=768,
                 cnn_channels=[64, 128, 192],
                 lstm_hidden=128,
                 lstm_layers=3,
                 attn_dim=128,
                 dropout=0.15,
                 num_classes=6):
        super().__init__()
        # CNN frontend: expects input [batch, time, feats]
        self.conv1 = nn.Sequential(
            nn.Conv1d(in_feats, cnn_channels[0], kernel_size=5, padding=2),
            nn.BatchNorm1d(cnn_channels[0]),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        self.conv2 = nn.Sequential(
            nn.Conv1d(cnn_channels[0], cnn_channels[1], kernel_size=3, padding=1),
            nn.BatchNorm1d(cnn_channels[1]),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        self.conv3 = nn.Sequential(
            nn.Conv1d(cnn_channels[1], cnn_channels[2], kernel_size=3, padding=1),
            nn.BatchNorm1d(cnn_channels[2]),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        # Bi-LSTM
        self.lstm = nn.LSTM(
            input_size=cnn_channels[2],
            hidden_size=lstm_hidden,
            num_layers=lstm_layers,
            batch_first=True,
            dropout=dropout,
            bidirectional=True
        )

        # Attention mechanism
        self.attn_proj = nn.Linear(2 * lstm_hidden, attn_dim)
        self.attn_score = nn.Linear(attn_dim, 1)

        # Classifier
        self.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(2 * lstm_hidden, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        # x: [batch, time, feats]
        x = x.transpose(1, 2)  # to [batch, feats, time] for Conv1d
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = x.transpose(1, 2)  # back to [batch, time, feats] for LSTM

        h, _ = self.lstm(x)    # [batch, time, 2*lstm_hidden]

        # Attention pooling
        proj = torch.tanh(self.attn_proj(h))
        scores = self.attn_score(proj).squeeze(-1)         # [batch, time]
        weights = torch.softmax(scores, dim=1).unsqueeze(-1) # [batch, time, 1]
        context = torch.sum(h * weights, dim=1)            # [batch, 2*lstm_hidden]

        out = self.classifier(context)
        return out

class MLPClassifier(nn.Module):
    def __init__(self, in_feats=768, hidden_dim=128, dropout=0.2, num_classes=6,):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_feats, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, num_classes)
        )
    def forward(self, x):
        return self.net(x)

#Train the model

# Use the updated EmotionClassifier model
model = MLPClassifier(in_feats = 768, # Input size is the dimension of mean-pooled features
                      hidden_dim = 128,
                      dropout = 0.15,
                      num_classes = len(np.unique(labels_encoded))).to(device)

# Use nn.CrossEntropyLoss for loss calculation
criterion = nn.CrossEntropyLoss(weight = class_weights) # Use CrossEntropyLoss with class weights

optimizer = optim.AdamW(model.parameters(), lr = 1e-3, weight_decay= 1e-7)
epochs = 80
min_delta = 0.005
patience = 80
scheduler = OneCycleLR(optimizer, max_lr = 1e-3,
                       steps_per_epoch = len(train_loader),
                       epochs = epochs,
                       pct_start = 0.3,
                       div_factor = 10,
                       final_div_factor = 100)

#Training Loop
loss_history = []
val_loss_history = []
best_val_loss = float('inf')
counter = 0
correct_train = 0
total_train = 0
best_model_state = None

for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    for inputs, labels in train_loader:
        # Skip batch if it has only one sample, which causes issues with BatchNorm
        if inputs.size(0) == 1:
            continue

        inputs = inputs.to(device)
        # Convert labels to one-hot encoding and move to device
        labels_one_hot = torch.nn.functional.one_hot(labels, num_classes=len(np.unique(labels_encoded))).float().to(device)


        optimizer.zero_grad()
        outputs = model(inputs)

        # Use CrossEntropyLoss with raw outputs and one-hot encoded labels
        loss = criterion(outputs, labels_one_hot)

        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 2.0)

        optimizer.step()
        scheduler.step()
        running_loss += loss.item()

        #Count correct predictions (use original labels for accuracy)
        _, predicted = torch.max(outputs.data, 1)
        total_train += labels.size(0)
        correct_train += (predicted == labels.to(device)).sum().item() # Compare with original labels

    avg_loss = running_loss/len(train_loader)
    train_acc = 100 * correct_train / total_train
    loss_history.append(avg_loss)
    #Validation
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for val_inputs, val_labels in val_loader:
            val_inputs = val_inputs.to(device)
            # Convert validation labels to one-hot encoding and move to device
            val_labels_one_hot = torch.nn.functional.one_hot(val_labels, num_classes=len(np.unique(labels_encoded))).float().to(device)
            val_outputs = model(val_inputs)

            # Use CrossEntropyLoss for validation loss with one-hot encoded labels
            v_loss = criterion(val_outputs, val_labels_one_hot)

            val_loss += v_loss.item()
    avg_val_loss = val_loss/len(val_loader)
    val_loss_history.append(avg_val_loss)
    print(f"Epoch {epoch+1}, Training Loss: {avg_loss:.4f}, Training Acc: {train_acc:.2f}%, Validation Loss: {avg_val_loss:.4f}")

    #Early Stopping
    if best_val_loss - avg_val_loss > min_delta:
        best_val_loss = avg_val_loss
        patience_counter = 0
        torch.save(model.state_dict(), 'best_model.pth')
        print(f"New Best Model save at epoch {epoch+1}")

    else:
        patience_counter += 1
        print(f"No improvement in validation loss for {patience_counter} epochs.")
        if patience_counter >= patience:
            print(f"Early stopping at epoch {epoch+1}.")
            break

#Evaluate Model on Test Set
model.load_state_dict(torch.load('best_model.pth'))
model.eval()
correct = 0
total = 0
all_preds = []
all_labels = []

with torch.no_grad():
    for inputs, labels in test_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        all_preds.extend(predicted.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())
print(f"Final Training Accuracy: {train_acc:.2f}%")
print(f"Test Accuracy: {100 * correct/total:.2f}%")
all_preds = np.array(all_preds)
all_labels = np.array(all_labels)
print("\nPer-Emotion Accuracy:")
for idx, emotion in enumerate(le.classes_):
    idx_mask = all_labels == idx
    emotion_acc = accuracy_score(all_labels[idx_mask], all_preds[idx_mask])
    print(f"{emotion.capitalize():<10}: {emotion_acc*100:.2f}")

print("\nClassification Report:")
print(classification_report(all_labels, all_preds, target_names = le.classes_))
f1 = f1_score(all_labels, all_preds, average = 'macro')
print(f"F1 Score: {f1:.4f}")

#Plot Training Loss
plt.figure(figsize = (8,5))
plt.plot(loss_history, marker = 'o', color = 'magenta')
plt.plot(val_loss_history, label = 'Validation Loss', color = 'purple', marker = 'x')
plt.title('Training Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)
plt.show()

#Confusion Matrix
#Get Predictions
all_preds = []
all_labels = []

model.eval()
with torch.no_grad():
    for inputs, labels in test_loader:
        inputs = inputs.to(device)
        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.numpy())

#Calculate Confusion Matrix
cm = confusion_matrix(all_labels, all_preds)
disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = le.classes_)

plt.figure(figsize = (8,6))
disp.plot(cmap = plt.cm.Blues, xticks_rotation = 45)
plt.title('Confusion Matrix')
plt.show()